<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <meta name="generator" content="Hugo 0.145.0"><meta property="og:url" content="https://www.mikeperham.com/2019/12/17/workload-isolation-with-queue-sharding/">
  <meta property="og:site_name" content="Mike Perham">
  <meta property="og:title" content="Workload Isolation with Queue Sharding">
  <meta property="og:description" content="A number of customers have contacted me with a common problem:
We run a multi-tenant system where our users can perform an action which results in a huge number of jobs being enqueued. When this happens, other users see significant delays in their jobs being processed while our Sidekiq cluster works through the backlog for that one user.
The issue is this: if you give 100% of your resources to process that user‚Äôs backlog, 100% of your customers will feel the pain of that backlog delay. For years I‚Äôve recommended customers use a simple setup with three queues: critical, default and bulk. If user A throws 100,000 jobs into bulk, processing those jobs would be low priority and perhaps take one or two hours. That backlog will not block critical or default jobs but it will delay any bulk processing by other users.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2019-12-17T10:55:37-08:00">
    <meta property="article:modified_time" content="2019-12-17T10:55:37-08:00">

  <meta name="description" content="Ruby, OSS and the Internet">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://www.mikeperham.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://www.mikeperham.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://www.mikeperham.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://www.mikeperham.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://www.mikeperham.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="/css/bootstrap.min.css" integrity="sha384-zCbKRCUGaJDkqS1kPbPd7TveP5iyJE0EjAuZQTgFLD2ylzuqKfdKlfG/eSrtxUkn" crossorigin="anonymous">
  <link rel="alternate" type="application/rss+xml" href="https://mikeperham.com/index.xml" title="Mike Perham">

  
  <title>Workload Isolation with Queue Sharding | Mike Perham</title>
  

  <style>
body {
  min-width: 300px;
  font-size: 19px;
}

.publish-date {
  font-family: monospace;
  font-size: 0.8em;
  margin-right: 0.5em;
}

.custom-navbar {
  margin-bottom: 1em;
}

@media print {
  .custom-navbar {
    display: none;
  }
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 960px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>

</head>


<body>
  <nav class="custom-navbar navbar navbar-expand-md navbar-dark bg-dark">
  <div class="container">
    <a class="navbar-brand mr-0" href="/">Mike Perham</a>
    <ul class="navbar-nav d-flex flex-row">
      <li class="nav-item"><a class="nav-link" title="Sidekiq" href="https://sidekiq.org">Sidekiqü•ã</a></li>
      <li class="nav-item"><a class="nav-link" title="Faktory" href="https://contribsys.com/faktory">Faktoryüè≠</a></li>
    </ul>
    <ul class="navbar-nav float-right d-none d-md-flex">
      
      <li class="nav-item"><a class="nav-link" href="mailto:mike@perham.net">üìß Email</a></li>
      <li class="nav-item"><a class="nav-link" rel="me" href="https://ruby.social/@getajobmike">üì£ Mastodon</a></li>
      <li class="nav-item"><a class="nav-link" href="/index.xml" title="RSS"><img src="/images/rss.png" width="16"
            height="16" /> RSS</a></li>
      <li class="nav-item d-none d-md-flex"><a class="nav-link" title="About" href="/about/">‚ùì</a></li>
    </ul>
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Workload Isolation with Queue Sharding</h1>
<p>
  <small class="text-secondary">
  
  
  2019-12-17
  </small>
  
</p>
<p>A number of customers have contacted me with a common problem:</p>
<blockquote>
<p>We run a multi-tenant system where our users can perform an action which
results in a huge number of jobs being enqueued. When this happens, other
users see significant delays in their jobs being processed while our
Sidekiq cluster works through the backlog for that one user.</p></blockquote>
<p>The issue is this: if you give 100% of your resources to process that
user&rsquo;s backlog, 100% of your customers will feel the pain of that backlog
delay. For years I&rsquo;ve recommended customers use a simple setup with three
queues: <code>critical</code>, <code>default</code> and <code>bulk</code>. If user A throws 100,000 jobs
into <code>bulk</code>, processing those jobs would be low priority and perhaps take
one or two hours.  That backlog will not block critical or default jobs <strong>but</strong> it will
delay any bulk processing by other users.</p>
<p>Generically, this issue is known as <code>workload isolation</code>. AWS, in
particular, has published a few articles on how they deal with this
problem in their services and they&rsquo;ve highlighted an interesting
technique used to provide isolation between customers.</p>
<p><img src="https://d1.awsstatic.com/legal/builders-library/Screenshots/shuffle-sharding-with-eight-workers.97e815152d06856351e6976ed33029414f1a7f99.png" alt="shuffle sharding example"></p>
<p>The key idea is known as <a href="https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/">shuffle sharding</a> but the technique is not specific to AWS, you can use it today with your Sidekiq cluster.
Go read that article, it&rsquo;s quite good.</p>
<h2 id="queues-and-processes">Queues and Processes</h2>
<p>We&rsquo;ll use real numbers here to minimize confusion but you can
adjust these numbers for your own scale.</p>
<p>Note also that this technique is completely separate from Redis
sharding.
If you have 4 Redis shards, you have 4 Sidekiq clusters.
This queue sharding technique is specific to a single Sidekiq cluster
running against a single Redis instance.</p>
<p>We assume that operations which trigger high job volumes will go into a logical <code>bulk</code>
queue but in reality your app will enqueue those jobs into eight shards: bulk0 - bulk7.</p>
<p>You have 8 Sidekiq processes and each process will process <code>critical</code>,
<code>default</code> and 2 bulk shards.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ruby" data-lang="ruby"><span style="display:flex;"><span>shards <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>
</span></span><span style="display:flex;"><span>shards<span style="color:#f92672">.</span>times <span style="color:#66d9ef">do</span> <span style="color:#f92672">|</span>idx<span style="color:#f92672">|</span>
</span></span><span style="display:flex;"><span>  other <span style="color:#f92672">=</span> idx<span style="color:#f92672">.</span>succ <span style="color:#f92672">%</span> shards
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;bundle exec sidekiq -q critical -q default -q bulk</span><span style="color:#e6db74">#{</span>idx<span style="color:#e6db74">}</span><span style="color:#e6db74"> -q bulk</span><span style="color:#e6db74">#{</span>other<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><p>Now each user&rsquo;s operation should dynamically target a bulk shard.
If you have a random ID for the overall operation, you can do something as simple as:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ruby" data-lang="ruby"><span style="display:flex;"><span>q <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;bulk</span><span style="color:#e6db74">#{</span>operationID <span style="color:#f92672">%</span> <span style="color:#ae81ff">8</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">100_000</span><span style="color:#f92672">.</span>times <span style="color:#66d9ef">do</span> <span style="color:#f92672">|</span>idx<span style="color:#f92672">|</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># push 100,000 jobs to a bulk shard</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">SomeWorker</span><span style="color:#f92672">.</span>set(<span style="color:#e6db74">queue</span>:q)<span style="color:#f92672">.</span>perform_async(idx)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><p>Other bulk user operations should randomly select a shard also. They have a 1 in 8 chance of selecting the same shard but most of the time the operations will be isolated from each other.
Net result: if a user operation creates a large number of bulk jobs, this will only affect 12%
of other operations rather than 100% as we were seeing at the beginning. The trade off is that only two Sidekiq processes will be processing any one bulk shard.</p>
<p>Want faster processing?  Spread the jobs across two random shards. You
get 100% more processes but only increase your odds from 12% to 25%
that you will disrupt anyone.</p>
<p>That trade off is the crux: you might want more processes or fewer
shards to get that backlog processed quicker. 32 processes and 8 shards
will get you 4 processes for a queue. 16 processes with 4 shards will
also get you 4 processes for a queue but will mean that 1/4 of
operations will clash instead of 1/8. Only you can judge what&rsquo;s
appropriate for your app and budget.</p>
<h2 id="conclusion">Conclusion</h2>
<p>By sharding the <code>bulk</code> queue, we isolate our Sidekiq resources into buckets so
that any one bulk user operation can&rsquo;t monopolize all resources.  Ultimately
this is a trade off because it also naturally limits the thoroughput of
those operations: they can&rsquo;t use the full set of resources in production
to finish quicker. High priority operations still have the option to
utilize more than one bulk shard or target the <code>default</code> queue in order to
blast through a backlog with 100% of your resources.</p>

    </article>
  </div>

  

</body>

</html>
